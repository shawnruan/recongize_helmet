{
    "timestamp": "2025-04-07T02:50:28.938107",
    "total_tests": 36,
    "models_tested": [
        "gemma3:27b",
        "gemma3:12b",
        "minicpm-v"
    ],
    "experiment_types": [
        "crop",
        "count"
    ],
    "prompt_configs": {
        "crop": [
            "prompts/test-prompts.md",
            "prompts/test-prompts-en.md"
        ],
        "count": [
            "prompts/count-prompts.md",
            "prompts/detect-prompts.md"
        ]
    },
    "reports": [
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.6976744186046512,
                "precision": 0.6313131313131313,
                "recall": 0.9615384615384616,
                "f1_score": 0.7621951219512195,
                "true_positives": 125,
                "false_positives": 73,
                "false_negatives": 5,
                "true_negatives": 55
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.9227642276422764,
                "precision": 0.9668508287292817,
                "recall": 0.9308510638297872,
                "f1_score": 0.9485094850948509,
                "true_positives": 175,
                "false_positives": 6,
                "false_negatives": 13,
                "true_negatives": 52
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.7513227513227513,
                "precision": 0.7872340425531915,
                "recall": 0.8671875,
                "f1_score": 0.825278810408922,
                "true_positives": 111,
                "false_positives": 30,
                "false_negatives": 17,
                "true_negatives": 31
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.6236559139784946,
                "precision": 0.8672566371681416,
                "recall": 0.6405228758169934,
                "f1_score": 0.7368421052631579,
                "true_positives": 98,
                "false_positives": 15,
                "false_negatives": 55,
                "true_negatives": 18
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.7054263565891473,
                "precision": 0.6483516483516484,
                "recall": 0.9076923076923077,
                "f1_score": 0.7564102564102564,
                "true_positives": 118,
                "false_positives": 64,
                "false_negatives": 12,
                "true_negatives": 64
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.8861788617886179,
                "precision": 0.9938271604938271,
                "recall": 0.8563829787234043,
                "f1_score": 0.92,
                "true_positives": 161,
                "false_positives": 1,
                "false_negatives": 27,
                "true_negatives": 57
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.6931216931216931,
                "precision": 0.8070175438596491,
                "recall": 0.71875,
                "f1_score": 0.7603305785123967,
                "true_positives": 92,
                "false_positives": 22,
                "false_negatives": 36,
                "true_negatives": 39
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.543010752688172,
                "precision": 0.9594594594594594,
                "recall": 0.46405228758169936,
                "f1_score": 0.6255506607929515,
                "true_positives": 71,
                "false_positives": 3,
                "false_negatives": 82,
                "true_negatives": 30
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.6821705426356589,
                "precision": 0.6188118811881188,
                "recall": 0.9615384615384616,
                "f1_score": 0.7530120481927711,
                "true_positives": 125,
                "false_positives": 77,
                "false_negatives": 5,
                "true_negatives": 51
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.9186991869918699,
                "precision": 0.9615384615384616,
                "recall": 0.9308510638297872,
                "f1_score": 0.9459459459459459,
                "true_positives": 175,
                "false_positives": 7,
                "false_negatives": 13,
                "true_negatives": 51
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.7724867724867724,
                "precision": 0.785234899328859,
                "recall": 0.9140625,
                "f1_score": 0.8447653429602889,
                "true_positives": 117,
                "false_positives": 32,
                "false_negatives": 11,
                "true_negatives": 29
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.7258064516129032,
                "precision": 0.8923076923076924,
                "recall": 0.7581699346405228,
                "f1_score": 0.8197879858657244,
                "true_positives": 116,
                "false_positives": 14,
                "false_negatives": 37,
                "true_negatives": 19
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.44573643410852715,
                "precision": 0.4057971014492754,
                "recall": 0.2153846153846154,
                "f1_score": 0.2814070351758794,
                "true_positives": 28,
                "false_positives": 41,
                "false_negatives": 102,
                "true_negatives": 87
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.7032520325203252,
                "precision": 0.9078014184397163,
                "recall": 0.6808510638297872,
                "f1_score": 0.7781155015197568,
                "true_positives": 128,
                "false_positives": 13,
                "false_negatives": 60,
                "true_negatives": 45
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.671957671957672,
                "precision": 0.746268656716418,
                "recall": 0.78125,
                "f1_score": 0.7633587786259542,
                "true_positives": 100,
                "false_positives": 34,
                "false_negatives": 28,
                "true_negatives": 27
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.5376344086021505,
                "precision": 0.845360824742268,
                "recall": 0.5359477124183006,
                "f1_score": 0.656,
                "true_positives": 82,
                "false_positives": 15,
                "false_negatives": 71,
                "true_negatives": 18
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.6937984496124031,
                "precision": 0.6335078534031413,
                "recall": 0.9307692307692308,
                "f1_score": 0.7538940809968847,
                "true_positives": 121,
                "false_positives": 70,
                "false_negatives": 9,
                "true_negatives": 58
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.9186991869918699,
                "precision": 0.9615384615384616,
                "recall": 0.9308510638297872,
                "f1_score": 0.9459459459459459,
                "true_positives": 175,
                "false_positives": 7,
                "false_negatives": 13,
                "true_negatives": 51
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.7142857142857143,
                "precision": 0.8032786885245902,
                "recall": 0.765625,
                "f1_score": 0.784,
                "true_positives": 98,
                "false_positives": 24,
                "false_negatives": 30,
                "true_negatives": 37
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.5860215053763441,
                "precision": 0.9042553191489362,
                "recall": 0.5555555555555556,
                "f1_score": 0.6882591093117408,
                "true_positives": 85,
                "false_positives": 9,
                "false_negatives": 68,
                "true_negatives": 24
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.7170542635658915,
                "precision": 0.6574585635359116,
                "recall": 0.9153846153846154,
                "f1_score": 0.7652733118971061,
                "true_positives": 119,
                "false_positives": 62,
                "false_negatives": 11,
                "true_negatives": 66
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.8943089430894309,
                "precision": 1.0,
                "recall": 0.8617021276595744,
                "f1_score": 0.9257142857142857,
                "true_positives": 162,
                "false_positives": 0,
                "false_negatives": 26,
                "true_negatives": 58
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.6613756613756614,
                "precision": 0.8018867924528302,
                "recall": 0.6640625,
                "f1_score": 0.7264957264957265,
                "true_positives": 85,
                "false_positives": 21,
                "false_negatives": 43,
                "true_negatives": 40
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.3655913978494624,
                "precision": 1.0,
                "recall": 0.22875816993464052,
                "f1_score": 0.3723404255319149,
                "true_positives": 35,
                "false_positives": 0,
                "false_negatives": 118,
                "true_negatives": 33
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "gemma3:27b",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.48598130841121495,
                    "precision": 0.5148514851485149,
                    "recall": 0.896551724137931,
                    "f1_score": 0.6540880503144655,
                    "true_positives": 52,
                    "false_positives": 49,
                    "false_negatives": 6,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.6915422885572139,
                    "precision": 0.9144736842105263,
                    "recall": 0.7393617021276596,
                    "f1_score": 0.8176470588235294,
                    "true_positives": 139,
                    "false_positives": 13,
                    "false_negatives": 49,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.8447653429602888,
                    "precision": 0.924901185770751,
                    "recall": 0.9069767441860465,
                    "f1_score": 0.9158512720156555,
                    "true_positives": 234,
                    "false_positives": 19,
                    "false_negatives": 24,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.5,
                    "precision": 0.5087719298245614,
                    "recall": 0.9666666666666667,
                    "f1_score": 0.6666666666666667,
                    "true_positives": 29,
                    "false_positives": 28,
                    "false_negatives": 1,
                    "true_negatives": 22
                }
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "gemma3:27b",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.3333333333333333,
                    "precision": 0.4117647058823529,
                    "recall": 0.6363636363636364,
                    "f1_score": 0.5,
                    "true_positives": 21,
                    "false_positives": 30,
                    "false_negatives": 12,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.467005076142132,
                    "precision": 0.6764705882352942,
                    "recall": 0.6013071895424836,
                    "f1_score": 0.6366782006920416,
                    "true_positives": 92,
                    "false_positives": 44,
                    "false_negatives": 61,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.5666666666666667,
                    "precision": 0.7272727272727273,
                    "recall": 0.7195767195767195,
                    "f1_score": 0.7234042553191489,
                    "true_positives": 136,
                    "false_positives": 51,
                    "false_negatives": 53,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.3684210526315789,
                    "precision": 0.4666666666666667,
                    "recall": 0.6363636363636364,
                    "f1_score": 0.5384615384615385,
                    "true_positives": 21,
                    "false_positives": 24,
                    "false_negatives": 12,
                    "true_negatives": 23
                }
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "gemma3:27b",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.5113636363636364,
                    "precision": 0.6,
                    "recall": 0.7758620689655172,
                    "f1_score": 0.6766917293233082,
                    "true_positives": 45,
                    "false_positives": 30,
                    "false_negatives": 13,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.7575757575757576,
                    "precision": 0.9375,
                    "recall": 0.7978723404255319,
                    "f1_score": 0.8620689655172414,
                    "true_positives": 150,
                    "false_positives": 10,
                    "false_negatives": 38,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.8603773584905661,
                    "precision": 0.9702127659574468,
                    "recall": 0.8837209302325582,
                    "f1_score": 0.924949290060852,
                    "true_positives": 228,
                    "false_positives": 7,
                    "false_negatives": 30,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.5384615384615384,
                    "precision": 0.56,
                    "recall": 0.9333333333333333,
                    "f1_score": 0.7000000000000001,
                    "true_positives": 28,
                    "false_positives": 22,
                    "false_negatives": 2,
                    "true_negatives": 28
                }
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "gemma3:27b",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.3409090909090909,
                    "precision": 0.35294117647058826,
                    "recall": 0.9090909090909091,
                    "f1_score": 0.5084745762711865,
                    "true_positives": 30,
                    "false_positives": 55,
                    "false_negatives": 3,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.45685279187817257,
                    "precision": 0.6716417910447762,
                    "recall": 0.5882352941176471,
                    "f1_score": 0.6271777003484321,
                    "true_positives": 90,
                    "false_positives": 44,
                    "false_negatives": 63,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.516728624535316,
                    "precision": 0.634703196347032,
                    "recall": 0.7354497354497355,
                    "f1_score": 0.681372549019608,
                    "true_positives": 139,
                    "false_positives": 80,
                    "false_negatives": 50,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.42857142857142855,
                    "precision": 0.44776119402985076,
                    "recall": 0.9090909090909091,
                    "f1_score": 0.6,
                    "true_positives": 30,
                    "false_positives": 37,
                    "false_negatives": 3,
                    "true_negatives": 10
                }
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "gemma3:12b",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.35964912280701755,
                    "precision": 0.422680412371134,
                    "recall": 0.7068965517241379,
                    "f1_score": 0.529032258064516,
                    "true_positives": 41,
                    "false_positives": 56,
                    "false_negatives": 17,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.4479166666666667,
                    "precision": 0.9555555555555556,
                    "recall": 0.4574468085106383,
                    "f1_score": 0.6187050359712231,
                    "true_positives": 86,
                    "false_positives": 4,
                    "false_negatives": 102,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.7049808429118773,
                    "precision": 0.983957219251337,
                    "recall": 0.7131782945736435,
                    "f1_score": 0.8269662921348315,
                    "true_positives": 184,
                    "false_positives": 3,
                    "false_negatives": 74,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.421875,
                    "precision": 0.4426229508196721,
                    "recall": 0.9,
                    "f1_score": 0.5934065934065934,
                    "true_positives": 27,
                    "false_positives": 34,
                    "false_negatives": 3,
                    "true_negatives": 16
                }
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "gemma3:12b",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.29310344827586204,
                    "precision": 0.40476190476190477,
                    "recall": 0.5151515151515151,
                    "f1_score": 0.4533333333333333,
                    "true_positives": 17,
                    "false_positives": 25,
                    "false_negatives": 16,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.39805825242718446,
                    "precision": 0.6074074074074074,
                    "recall": 0.5359477124183006,
                    "f1_score": 0.5694444444444444,
                    "true_positives": 82,
                    "false_positives": 53,
                    "false_negatives": 71,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.512396694214876,
                    "precision": 0.7005649717514124,
                    "recall": 0.656084656084656,
                    "f1_score": 0.6775956284153005,
                    "true_positives": 124,
                    "false_positives": 53,
                    "false_negatives": 65,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.3541666666666667,
                    "precision": 0.53125,
                    "recall": 0.5151515151515151,
                    "f1_score": 0.5230769230769231,
                    "true_positives": 17,
                    "false_positives": 15,
                    "false_negatives": 16,
                    "true_negatives": 32
                }
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "gemma3:12b",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.3028169014084507,
                    "precision": 0.33858267716535434,
                    "recall": 0.7413793103448276,
                    "f1_score": 0.46486486486486484,
                    "true_positives": 43,
                    "false_positives": 84,
                    "false_negatives": 15,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.5116279069767442,
                    "precision": 0.8029197080291971,
                    "recall": 0.5851063829787234,
                    "f1_score": 0.676923076923077,
                    "true_positives": 110,
                    "false_positives": 27,
                    "false_negatives": 78,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.6948051948051948,
                    "precision": 0.8106060606060606,
                    "recall": 0.8294573643410853,
                    "f1_score": 0.8199233716475095,
                    "true_positives": 214,
                    "false_positives": 50,
                    "false_negatives": 44,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.379746835443038,
                    "precision": 0.379746835443038,
                    "recall": 1.0,
                    "f1_score": 0.5504587155963303,
                    "true_positives": 30,
                    "false_positives": 49,
                    "false_negatives": 0,
                    "true_negatives": 1
                }
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "gemma3:12b",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.2366412213740458,
                    "precision": 0.24031007751937986,
                    "recall": 0.9393939393939394,
                    "f1_score": 0.38271604938271603,
                    "true_positives": 31,
                    "false_positives": 98,
                    "false_negatives": 2,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.35276967930029157,
                    "precision": 0.3890675241157556,
                    "recall": 0.7908496732026143,
                    "f1_score": 0.5215517241379309,
                    "true_positives": 121,
                    "false_positives": 190,
                    "false_negatives": 32,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.36147186147186144,
                    "precision": 0.3795454545454545,
                    "recall": 0.8835978835978836,
                    "f1_score": 0.5310015898251192,
                    "true_positives": 167,
                    "false_positives": 273,
                    "false_negatives": 22,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.40789473684210525,
                    "precision": 0.4189189189189189,
                    "recall": 0.9393939393939394,
                    "f1_score": 0.5794392523364487,
                    "true_positives": 31,
                    "false_positives": 43,
                    "false_negatives": 2,
                    "true_negatives": 4
                }
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "minicpm-v",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.3669064748201439,
                    "precision": 0.38636363636363635,
                    "recall": 0.8793103448275862,
                    "f1_score": 0.5368421052631579,
                    "true_positives": 51,
                    "false_positives": 81,
                    "false_negatives": 7,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.8118811881188119,
                    "precision": 0.9213483146067416,
                    "recall": 0.8723404255319149,
                    "f1_score": 0.8961748633879781,
                    "true_positives": 164,
                    "false_positives": 14,
                    "false_negatives": 24,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.7308868501529052,
                    "precision": 0.775974025974026,
                    "recall": 0.9263565891472868,
                    "f1_score": 0.8445229681978798,
                    "true_positives": 239,
                    "false_positives": 69,
                    "false_negatives": 19,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.42857142857142855,
                    "precision": 0.48,
                    "recall": 0.8,
                    "f1_score": 0.6,
                    "true_positives": 24,
                    "false_positives": 26,
                    "false_negatives": 6,
                    "true_negatives": 24
                }
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "minicpm-v",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.18181818181818182,
                    "precision": 0.2413793103448276,
                    "recall": 0.42424242424242425,
                    "f1_score": 0.3076923076923077,
                    "true_positives": 14,
                    "false_positives": 44,
                    "false_negatives": 19,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.6,
                    "precision": 0.8571428571428571,
                    "recall": 0.6666666666666666,
                    "f1_score": 0.75,
                    "true_positives": 102,
                    "false_positives": 17,
                    "false_negatives": 51,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.4558011049723757,
                    "precision": 0.4881656804733728,
                    "recall": 0.873015873015873,
                    "f1_score": 0.6261859582542695,
                    "true_positives": 165,
                    "false_positives": 173,
                    "false_negatives": 24,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.26666666666666666,
                    "precision": 0.37209302325581395,
                    "recall": 0.48484848484848486,
                    "f1_score": 0.4210526315789474,
                    "true_positives": 16,
                    "false_positives": 27,
                    "false_negatives": 17,
                    "true_negatives": 20
                }
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "minicpm-v",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.22105263157894736,
                    "precision": 0.3620689655172414,
                    "recall": 0.3620689655172414,
                    "f1_score": 0.36206896551724144,
                    "true_positives": 21,
                    "false_positives": 37,
                    "false_negatives": 37,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.7652173913043478,
                    "precision": 0.8073394495412844,
                    "recall": 0.9361702127659575,
                    "f1_score": 0.8669950738916257,
                    "true_positives": 176,
                    "false_positives": 42,
                    "false_negatives": 12,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.8,
                    "precision": 0.8384879725085911,
                    "recall": 0.9457364341085271,
                    "f1_score": 0.888888888888889,
                    "true_positives": 244,
                    "false_positives": 47,
                    "false_negatives": 14,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.25,
                    "precision": 0.35,
                    "recall": 0.4666666666666667,
                    "f1_score": 0.4,
                    "true_positives": 14,
                    "false_positives": 26,
                    "false_negatives": 16,
                    "true_negatives": 24
                }
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "minicpm-v",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.27631578947368424,
                    "precision": 0.328125,
                    "recall": 0.6363636363636364,
                    "f1_score": 0.4329896907216495,
                    "true_positives": 21,
                    "false_positives": 43,
                    "false_negatives": 12,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.2504708097928437,
                    "precision": 0.2602739726027397,
                    "recall": 0.869281045751634,
                    "f1_score": 0.40060240963855426,
                    "true_positives": 133,
                    "false_positives": 378,
                    "false_negatives": 20,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.29252782193958665,
                    "precision": 0.2948717948717949,
                    "recall": 0.9735449735449735,
                    "f1_score": 0.4526445264452644,
                    "true_positives": 184,
                    "false_positives": 440,
                    "false_negatives": 5,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.36065573770491804,
                    "precision": 0.44,
                    "recall": 0.6666666666666666,
                    "f1_score": 0.5301204819277109,
                    "true_positives": 22,
                    "false_positives": 28,
                    "false_negatives": 11,
                    "true_negatives": 19
                }
            },
            "prompt_config": "detect-prompts"
        }
    ]
}