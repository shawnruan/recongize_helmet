{
    "timestamp": "2025-06-02T00:37:57.148435",
    "total_tests": 8,
    "models_tested": [
        "minicpm-v"
    ],
    "experiment_types": [
        "crop",
        "count",
        "binary"
    ],
    "prompt_configs": {
        "crop": [
            "prompts/test-prompts.md",
            "prompts/test-prompts-en.md"
        ],
        "count": [],
        "binary": []
    },
    "binary_threshold": 0.5,
    "timing_summary": {
        "total_inference_time": 7817.627,
        "total_samples": 1758,
        "average_inference_time": 4.447
    },
    "f1_macro_summary": {
        "dataset_scores": {
            "sample_person": {
                "chinese": 0.6820186818889465,
                "english": 0.7261488511488512,
                "average": 0.7040837665188988
            },
            "sample_head_helmet": {
                "chinese": 0.9029628330995793,
                "english": 0.8625478927203065,
                "average": 0.8827553629099429
            },
            "lng_person": {
                "chinese": 0.7313953488372094,
                "english": 0.6573684210526316,
                "average": 0.6943818849449205
            },
            "lng_head_helmet": {
                "chinese": 0.5361295546558704,
                "english": 0.36551803885291395,
                "average": 0.4508237967543922
            }
        },
        "model_scores": {
            "minicpm-v": {
                "chinese": 0.7131266046204014,
                "english": 0.6528958009436758
            }
        }
    },
    "distribution_plots": null,
    "reports": [
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.7054263565891473,
                "precision": 0.6363636363636364,
                "recall": 0.9692307692307692,
                "f1_score": 0.7682926829268293,
                "helmet_precision": 0.6363636363636364,
                "helmet_recall": 0.9692307692307692,
                "helmet_f1": 0.7682926829268293,
                "head_precision": 0.9333333333333333,
                "head_recall": 0.4375,
                "head_f1": 0.5957446808510638,
                "f1_macro": 0.6820186818889465,
                "true_positives": 126,
                "false_positives": 72,
                "false_negatives": 4,
                "true_negatives": 56
            },
            "timing": {
                "total_time": 1150.696,
                "total_inference_time": 1150.583,
                "average_inference_time": 4.46,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.926829268292683,
                "precision": 0.9722222222222222,
                "recall": 0.9308510638297872,
                "f1_score": 0.9510869565217391,
                "helmet_precision": 0.9722222222222222,
                "helmet_recall": 0.9308510638297872,
                "helmet_f1": 0.9510869565217391,
                "head_precision": 0.803030303030303,
                "head_recall": 0.9137931034482759,
                "head_f1": 0.8548387096774194,
                "f1_macro": 0.9029628330995793,
                "true_positives": 175,
                "false_positives": 5,
                "false_negatives": 13,
                "true_negatives": 53
            },
            "timing": {
                "total_time": 882.537,
                "total_inference_time": 882.435,
                "average_inference_time": 3.587,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.7671957671957672,
                "precision": 0.823076923076923,
                "recall": 0.8359375,
                "f1_score": 0.8294573643410853,
                "helmet_precision": 0.823076923076923,
                "helmet_recall": 0.8359375,
                "helmet_f1": 0.8294573643410853,
                "head_precision": 0.6440677966101694,
                "head_recall": 0.6229508196721312,
                "head_f1": 0.6333333333333334,
                "f1_macro": 0.7313953488372094,
                "true_positives": 107,
                "false_positives": 23,
                "false_negatives": 21,
                "true_negatives": 38
            },
            "timing": {
                "total_time": 650.296,
                "total_inference_time": 650.211,
                "average_inference_time": 3.44,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.5860215053763441,
                "precision": 0.9042553191489362,
                "recall": 0.5555555555555556,
                "f1_score": 0.6882591093117408,
                "helmet_precision": 0.9042553191489362,
                "helmet_recall": 0.5555555555555556,
                "helmet_f1": 0.6882591093117408,
                "head_precision": 0.2608695652173913,
                "head_recall": 0.7272727272727273,
                "head_f1": 0.38399999999999995,
                "f1_macro": 0.5361295546558704,
                "true_positives": 85,
                "false_positives": 9,
                "false_negatives": 68,
                "true_negatives": 24
            },
            "timing": {
                "total_time": 638.882,
                "total_inference_time": 638.81,
                "average_inference_time": 3.434,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.7364341085271318,
                "precision": 0.6741573033707865,
                "recall": 0.9230769230769231,
                "f1_score": 0.7792207792207791,
                "helmet_precision": 0.6741573033707865,
                "helmet_recall": 0.9230769230769231,
                "helmet_f1": 0.7792207792207791,
                "head_precision": 0.875,
                "head_recall": 0.546875,
                "head_f1": 0.6730769230769231,
                "f1_macro": 0.7261488511488512,
                "true_positives": 120,
                "false_positives": 58,
                "false_negatives": 10,
                "true_negatives": 70
            },
            "timing": {
                "total_time": 1701.121,
                "total_inference_time": 1700.981,
                "average_inference_time": 6.593,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.8861788617886179,
                "precision": 1.0,
                "recall": 0.851063829787234,
                "f1_score": 0.9195402298850576,
                "helmet_precision": 1.0,
                "helmet_recall": 0.851063829787234,
                "helmet_f1": 0.9195402298850576,
                "head_precision": 0.6744186046511628,
                "head_recall": 1.0,
                "head_f1": 0.8055555555555556,
                "f1_macro": 0.8625478927203065,
                "true_positives": 160,
                "false_positives": 0,
                "false_negatives": 28,
                "true_negatives": 58
            },
            "timing": {
                "total_time": 1047.582,
                "total_inference_time": 1047.501,
                "average_inference_time": 4.258,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.671957671957672,
                "precision": 0.83,
                "recall": 0.6484375,
                "f1_score": 0.7280701754385964,
                "helmet_precision": 0.83,
                "helmet_recall": 0.6484375,
                "helmet_f1": 0.7280701754385964,
                "head_precision": 0.4943820224719101,
                "head_recall": 0.7213114754098361,
                "head_f1": 0.5866666666666667,
                "f1_macro": 0.6573684210526316,
                "true_positives": 83,
                "false_positives": 17,
                "false_negatives": 45,
                "true_negatives": 44
            },
            "timing": {
                "total_time": 921.345,
                "total_inference_time": 921.275,
                "average_inference_time": 4.874,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.3655913978494624,
                "precision": 1.0,
                "recall": 0.22875816993464052,
                "f1_score": 0.3723404255319149,
                "helmet_precision": 1.0,
                "helmet_recall": 0.22875816993464052,
                "helmet_f1": 0.3723404255319149,
                "head_precision": 0.2185430463576159,
                "head_recall": 1.0,
                "head_f1": 0.358695652173913,
                "f1_macro": 0.36551803885291395,
                "true_positives": 35,
                "false_positives": 0,
                "false_negatives": 118,
                "true_negatives": 33
            },
            "timing": {
                "total_time": 825.929,
                "total_inference_time": 825.831,
                "average_inference_time": 4.44,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts-en"
        }
    ]
}