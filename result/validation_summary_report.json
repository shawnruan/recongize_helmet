{
    "timestamp": "2025-04-06T07:35:42.192659",
    "total_tests": 12,
    "models_tested": [
        "minicpm-v"
    ],
    "experiment_types": [
        "crop",
        "count"
    ],
    "prompt_configs": {
        "crop": [
            "prompts/test-prompts.md",
            "prompts/test-prompts-en.md"
        ],
        "count": [
            "prompts/count-prompts.md",
            "prompts/detect-prompts.md"
        ]
    },
    "reports": [
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.6976744186046512,
                "precision": 0.6326530612244898,
                "recall": 0.9538461538461539,
                "f1_score": 0.7607361963190183,
                "true_positives": 124,
                "false_positives": 72,
                "false_negatives": 6,
                "true_negatives": 56
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.9146341463414634,
                "precision": 0.9771428571428571,
                "recall": 0.9095744680851063,
                "f1_score": 0.9421487603305784,
                "true_positives": 171,
                "false_positives": 4,
                "false_negatives": 17,
                "true_negatives": 54
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.6984126984126984,
                "precision": 0.7751937984496124,
                "recall": 0.78125,
                "f1_score": 0.7782101167315176,
                "true_positives": 100,
                "false_positives": 29,
                "false_negatives": 28,
                "true_negatives": 32
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.6290322580645161,
                "precision": 0.8888888888888888,
                "recall": 0.6274509803921569,
                "f1_score": 0.735632183908046,
                "true_positives": 96,
                "false_positives": 12,
                "false_negatives": 57,
                "true_negatives": 21
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.7248062015503876,
                "precision": 0.6594594594594595,
                "recall": 0.9384615384615385,
                "f1_score": 0.7746031746031745,
                "true_positives": 122,
                "false_positives": 63,
                "false_negatives": 8,
                "true_negatives": 65
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.8821138211382114,
                "precision": 0.9937888198757764,
                "recall": 0.851063829787234,
                "f1_score": 0.9169054441260746,
                "true_positives": 160,
                "false_positives": 1,
                "false_negatives": 28,
                "true_negatives": 57
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.6772486772486772,
                "precision": 0.845360824742268,
                "recall": 0.640625,
                "f1_score": 0.7288888888888888,
                "true_positives": 82,
                "false_positives": 15,
                "false_negatives": 46,
                "true_negatives": 46
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.3817204301075269,
                "precision": 1.0,
                "recall": 0.24836601307189543,
                "f1_score": 0.3979057591623037,
                "true_positives": 38,
                "false_positives": 0,
                "false_negatives": 115,
                "true_negatives": 33
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "minicpm-v",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.42276422764227645,
                    "precision": 0.4444444444444444,
                    "recall": 0.896551724137931,
                    "f1_score": 0.5942857142857143,
                    "true_positives": 52,
                    "false_positives": 65,
                    "false_negatives": 6,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.7722772277227723,
                    "precision": 0.9176470588235294,
                    "recall": 0.8297872340425532,
                    "f1_score": 0.871508379888268,
                    "true_positives": 156,
                    "false_positives": 14,
                    "false_negatives": 32,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.7587301587301587,
                    "precision": 0.8074324324324325,
                    "recall": 0.9263565891472868,
                    "f1_score": 0.8628158844765343,
                    "true_positives": 239,
                    "false_positives": 57,
                    "false_negatives": 19,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.5098039215686274,
                    "precision": 0.5531914893617021,
                    "recall": 0.8666666666666667,
                    "f1_score": 0.6753246753246753,
                    "true_positives": 26,
                    "false_positives": 21,
                    "false_negatives": 4,
                    "true_negatives": 29
                }
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "minicpm-v",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.15476190476190477,
                    "precision": 0.203125,
                    "recall": 0.3939393939393939,
                    "f1_score": 0.26804123711340205,
                    "true_positives": 13,
                    "false_positives": 51,
                    "false_negatives": 20,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.6337209302325582,
                    "precision": 0.8515625,
                    "recall": 0.7124183006535948,
                    "f1_score": 0.7758007117437722,
                    "true_positives": 109,
                    "false_positives": 19,
                    "false_negatives": 44,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.6602316602316602,
                    "precision": 0.7095435684647303,
                    "recall": 0.9047619047619048,
                    "f1_score": 0.7953488372093024,
                    "true_positives": 171,
                    "false_positives": 70,
                    "false_negatives": 18,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.23529411764705882,
                    "precision": 0.3137254901960784,
                    "recall": 0.48484848484848486,
                    "f1_score": 0.380952380952381,
                    "true_positives": 16,
                    "false_positives": 35,
                    "false_negatives": 17,
                    "true_negatives": 12
                }
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "minicpm-v",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.24242424242424243,
                    "precision": 0.36923076923076925,
                    "recall": 0.41379310344827586,
                    "f1_score": 0.39024390243902446,
                    "true_positives": 24,
                    "false_positives": 41,
                    "false_negatives": 34,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.7777777777777778,
                    "precision": 0.8254716981132075,
                    "recall": 0.9308510638297872,
                    "f1_score": 0.8749999999999999,
                    "true_positives": 175,
                    "false_positives": 37,
                    "false_negatives": 13,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.7756410256410257,
                    "precision": 0.8175675675675675,
                    "recall": 0.937984496124031,
                    "f1_score": 0.8736462093862817,
                    "true_positives": 242,
                    "false_positives": 54,
                    "false_negatives": 16,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.3269230769230769,
                    "precision": 0.4358974358974359,
                    "recall": 0.5666666666666667,
                    "f1_score": 0.4927536231884058,
                    "true_positives": 17,
                    "false_positives": 22,
                    "false_negatives": 13,
                    "true_negatives": 28
                }
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "minicpm-v",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.2727272727272727,
                    "precision": 0.3230769230769231,
                    "recall": 0.6363636363636364,
                    "f1_score": 0.4285714285714286,
                    "true_positives": 21,
                    "false_positives": 44,
                    "false_negatives": 12,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.2693877551020408,
                    "precision": 0.2814498933901919,
                    "recall": 0.8627450980392157,
                    "f1_score": 0.42443729903536975,
                    "true_positives": 132,
                    "false_positives": 337,
                    "false_negatives": 21,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.28434504792332266,
                    "precision": 0.2894308943089431,
                    "recall": 0.9417989417989417,
                    "f1_score": 0.4427860696517413,
                    "true_positives": 178,
                    "false_positives": 437,
                    "false_negatives": 11,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.3387096774193548,
                    "precision": 0.42,
                    "recall": 0.6363636363636364,
                    "f1_score": 0.5060240963855421,
                    "true_positives": 21,
                    "false_positives": 29,
                    "false_negatives": 12,
                    "true_negatives": 18
                }
            },
            "prompt_config": "detect-prompts"
        }
    ]
}