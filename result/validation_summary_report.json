{
    "timestamp": "2025-04-24T16:21:46.340162",
    "total_tests": 96,
    "models_tested": [
        "deepseek-janus",
        "qwen2.5-vl-32b",
        "minicpm-o",
        "minicpm-v",
        "deepseek-r1:7b",
        "deepseek-r1:32b",
        "gemma3:27b",
        "gemma3:12b"
    ],
    "experiment_types": [
        "crop",
        "count"
    ],
    "prompt_configs": {
        "crop": [
            "prompts/test-prompts.md",
            "prompts/test-prompts-en.md"
        ],
        "count": [
            "prompts/count-prompts.md",
            "prompts/detect-prompts.md"
        ]
    },
    "timing_summary": {
        "total_inference_time": 56608.482,
        "total_samples": 16624,
        "average_inference_time": 3.405
    },
    "reports": [
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "deepseek-janus",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.49224806201550386,
                "precision": 0.49696969696969695,
                "recall": 0.6307692307692307,
                "f1_score": 0.5559322033898305,
                "true_positives": 82,
                "false_positives": 83,
                "false_negatives": 48,
                "true_negatives": 45
            },
            "timing": {
                "total_time": 888.765,
                "total_inference_time": 888.582,
                "average_inference_time": 3.444,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "deepseek-janus",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.6016260162601627,
                "precision": 0.7884615384615384,
                "recall": 0.6542553191489362,
                "f1_score": 0.7151162790697675,
                "true_positives": 123,
                "false_positives": 33,
                "false_negatives": 65,
                "true_negatives": 25
            },
            "timing": {
                "total_time": 844.569,
                "total_inference_time": 844.4,
                "average_inference_time": 3.433,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "deepseek-janus",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.5291005291005291,
                "precision": 0.6756756756756757,
                "recall": 0.5859375,
                "f1_score": 0.6276150627615064,
                "true_positives": 75,
                "false_positives": 36,
                "false_negatives": 53,
                "true_negatives": 25
            },
            "timing": {
                "total_time": 642.659,
                "total_inference_time": 642.526,
                "average_inference_time": 3.4,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "deepseek-janus",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.5860215053763441,
                "precision": 0.8392857142857143,
                "recall": 0.6143790849673203,
                "f1_score": 0.7094339622641509,
                "true_positives": 94,
                "false_positives": 18,
                "false_negatives": 59,
                "true_negatives": 15
            },
            "timing": {
                "total_time": 623.08,
                "total_inference_time": 622.952,
                "average_inference_time": 3.349,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "deepseek-janus",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.4883720930232558,
                "precision": 0.4928571428571429,
                "recall": 0.5307692307692308,
                "f1_score": 0.5111111111111112,
                "true_positives": 69,
                "false_positives": 71,
                "false_negatives": 61,
                "true_negatives": 57
            },
            "timing": {
                "total_time": 848.985,
                "total_inference_time": 848.823,
                "average_inference_time": 3.29,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "deepseek-janus",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.5650406504065041,
                "precision": 0.7647058823529411,
                "recall": 0.6223404255319149,
                "f1_score": 0.6862170087976539,
                "true_positives": 117,
                "false_positives": 36,
                "false_negatives": 71,
                "true_negatives": 22
            },
            "timing": {
                "total_time": 809.838,
                "total_inference_time": 809.688,
                "average_inference_time": 3.291,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "deepseek-janus",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.48677248677248675,
                "precision": 0.6504854368932039,
                "recall": 0.5234375,
                "f1_score": 0.58008658008658,
                "true_positives": 67,
                "false_positives": 36,
                "false_negatives": 61,
                "true_negatives": 25
            },
            "timing": {
                "total_time": 623.029,
                "total_inference_time": 622.927,
                "average_inference_time": 3.296,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "deepseek-janus",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.46774193548387094,
                "precision": 0.7755102040816326,
                "recall": 0.49673202614379086,
                "f1_score": 0.6055776892430279,
                "true_positives": 76,
                "false_positives": 22,
                "false_negatives": 77,
                "true_negatives": 11
            },
            "timing": {
                "total_time": 610.281,
                "total_inference_time": 610.161,
                "average_inference_time": 3.28,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "qwen2.5-vl-32b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.4728682170542636,
                "precision": 0.47058823529411764,
                "recall": 0.36923076923076925,
                "f1_score": 0.4137931034482759,
                "true_positives": 48,
                "false_positives": 54,
                "false_negatives": 82,
                "true_negatives": 74
            },
            "timing": {
                "total_time": 1023.512,
                "total_inference_time": 1023.348,
                "average_inference_time": 3.966,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "qwen2.5-vl-32b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.45528455284552843,
                "precision": 0.7934782608695652,
                "recall": 0.3882978723404255,
                "f1_score": 0.5214285714285714,
                "true_positives": 73,
                "false_positives": 19,
                "false_negatives": 115,
                "true_negatives": 39
            },
            "timing": {
                "total_time": 836.946,
                "total_inference_time": 836.807,
                "average_inference_time": 3.402,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "qwen2.5-vl-32b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.4656084656084656,
                "precision": 0.6626506024096386,
                "recall": 0.4296875,
                "f1_score": 0.5213270142180095,
                "true_positives": 55,
                "false_positives": 28,
                "false_negatives": 73,
                "true_negatives": 33
            },
            "timing": {
                "total_time": 643.245,
                "total_inference_time": 643.128,
                "average_inference_time": 3.403,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "qwen2.5-vl-32b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.44086021505376344,
                "precision": 0.8266666666666667,
                "recall": 0.40522875816993464,
                "f1_score": 0.5438596491228072,
                "true_positives": 62,
                "false_positives": 13,
                "false_negatives": 91,
                "true_negatives": 20
            },
            "timing": {
                "total_time": 632.742,
                "total_inference_time": 632.629,
                "average_inference_time": 3.401,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "qwen2.5-vl-32b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.5232558139534884,
                "precision": 0.5454545454545454,
                "recall": 0.3230769230769231,
                "f1_score": 0.4057971014492754,
                "true_positives": 42,
                "false_positives": 35,
                "false_negatives": 88,
                "true_negatives": 93
            },
            "timing": {
                "total_time": 1266.469,
                "total_inference_time": 1266.322,
                "average_inference_time": 4.908,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "qwen2.5-vl-32b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.3780487804878049,
                "precision": 0.7536231884057971,
                "recall": 0.2765957446808511,
                "f1_score": 0.4046692607003891,
                "true_positives": 52,
                "false_positives": 17,
                "false_negatives": 136,
                "true_negatives": 41
            },
            "timing": {
                "total_time": 1251.515,
                "total_inference_time": 1251.371,
                "average_inference_time": 5.087,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "qwen2.5-vl-32b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.36507936507936506,
                "precision": 0.5833333333333334,
                "recall": 0.21875,
                "f1_score": 0.31818181818181823,
                "true_positives": 28,
                "false_positives": 20,
                "false_negatives": 100,
                "true_negatives": 41
            },
            "timing": {
                "total_time": 942.34,
                "total_inference_time": 942.226,
                "average_inference_time": 4.985,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "qwen2.5-vl-32b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.3333333333333333,
                "precision": 0.7959183673469388,
                "recall": 0.2549019607843137,
                "f1_score": 0.3861386138613861,
                "true_positives": 39,
                "false_positives": 10,
                "false_negatives": 114,
                "true_negatives": 23
            },
            "timing": {
                "total_time": 900.996,
                "total_inference_time": 900.878,
                "average_inference_time": 4.843,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "minicpm-o",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.7015503875968992,
                "precision": 0.6331658291457286,
                "recall": 0.9692307692307692,
                "f1_score": 0.7659574468085106,
                "true_positives": 126,
                "false_positives": 73,
                "false_negatives": 4,
                "true_negatives": 55
            },
            "timing": {
                "total_time": 804.713,
                "total_inference_time": 804.534,
                "average_inference_time": 3.118,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "minicpm-o",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.926829268292683,
                "precision": 0.967032967032967,
                "recall": 0.9361702127659575,
                "f1_score": 0.9513513513513513,
                "true_positives": 176,
                "false_positives": 6,
                "false_negatives": 12,
                "true_negatives": 52
            },
            "timing": {
                "total_time": 675.339,
                "total_inference_time": 675.176,
                "average_inference_time": 2.745,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "minicpm-o",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.7513227513227513,
                "precision": 0.7956204379562044,
                "recall": 0.8515625,
                "f1_score": 0.8226415094339622,
                "true_positives": 109,
                "false_positives": 28,
                "false_negatives": 19,
                "true_negatives": 33
            },
            "timing": {
                "total_time": 517.592,
                "total_inference_time": 517.459,
                "average_inference_time": 2.738,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "minicpm-o",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.6881720430107527,
                "precision": 0.8991596638655462,
                "recall": 0.6993464052287581,
                "f1_score": 0.7867647058823529,
                "true_positives": 107,
                "false_positives": 12,
                "false_negatives": 46,
                "true_negatives": 21
            },
            "timing": {
                "total_time": 510.233,
                "total_inference_time": 510.106,
                "average_inference_time": 2.743,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "minicpm-o",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.6976744186046512,
                "precision": 0.63,
                "recall": 0.9692307692307692,
                "f1_score": 0.7636363636363637,
                "true_positives": 126,
                "false_positives": 74,
                "false_negatives": 4,
                "true_negatives": 54
            },
            "timing": {
                "total_time": 746.26,
                "total_inference_time": 746.107,
                "average_inference_time": 2.892,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "minicpm-o",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.9308943089430894,
                "precision": 0.9476439790575916,
                "recall": 0.9627659574468085,
                "f1_score": 0.9551451187335092,
                "true_positives": 181,
                "false_positives": 10,
                "false_negatives": 7,
                "true_negatives": 48
            },
            "timing": {
                "total_time": 687.376,
                "total_inference_time": 687.235,
                "average_inference_time": 2.794,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "minicpm-o",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.7671957671957672,
                "precision": 0.795774647887324,
                "recall": 0.8828125,
                "f1_score": 0.8370370370370371,
                "true_positives": 113,
                "false_positives": 29,
                "false_negatives": 15,
                "true_negatives": 32
            },
            "timing": {
                "total_time": 526.205,
                "total_inference_time": 526.085,
                "average_inference_time": 2.784,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "minicpm-o",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.7365591397849462,
                "precision": 0.8768115942028986,
                "recall": 0.7908496732026143,
                "f1_score": 0.831615120274914,
                "true_positives": 121,
                "false_positives": 17,
                "false_negatives": 32,
                "true_negatives": 16
            },
            "timing": {
                "total_time": 519.399,
                "total_inference_time": 519.281,
                "average_inference_time": 2.792,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.7170542635658915,
                "precision": 0.6492146596858639,
                "recall": 0.9538461538461539,
                "f1_score": 0.7725856697819313,
                "true_positives": 124,
                "false_positives": 67,
                "false_negatives": 6,
                "true_negatives": 61
            },
            "timing": {
                "total_time": 744.119,
                "total_inference_time": 743.943,
                "average_inference_time": 2.884,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.9105691056910569,
                "precision": 0.9770114942528736,
                "recall": 0.9042553191489362,
                "f1_score": 0.9392265193370166,
                "true_positives": 170,
                "false_positives": 4,
                "false_negatives": 18,
                "true_negatives": 54
            },
            "timing": {
                "total_time": 654.922,
                "total_inference_time": 654.772,
                "average_inference_time": 2.662,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.7407407407407407,
                "precision": 0.8015267175572519,
                "recall": 0.8203125,
                "f1_score": 0.8108108108108109,
                "true_positives": 105,
                "false_positives": 26,
                "false_negatives": 23,
                "true_negatives": 35
            },
            "timing": {
                "total_time": 503.089,
                "total_inference_time": 502.971,
                "average_inference_time": 2.661,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.6559139784946236,
                "precision": 0.9238095238095239,
                "recall": 0.6339869281045751,
                "f1_score": 0.751937984496124,
                "true_positives": 97,
                "false_positives": 8,
                "false_negatives": 56,
                "true_negatives": 25
            },
            "timing": {
                "total_time": 495.112,
                "total_inference_time": 494.993,
                "average_inference_time": 2.661,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.7131782945736435,
                "precision": 0.6505376344086021,
                "recall": 0.9307692307692308,
                "f1_score": 0.7658227848101266,
                "true_positives": 121,
                "false_positives": 65,
                "false_negatives": 9,
                "true_negatives": 63
            },
            "timing": {
                "total_time": 719.143,
                "total_inference_time": 718.986,
                "average_inference_time": 2.787,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.8739837398373984,
                "precision": 1.0,
                "recall": 0.8351063829787234,
                "f1_score": 0.9101449275362319,
                "true_positives": 157,
                "false_positives": 0,
                "false_negatives": 31,
                "true_negatives": 58
            },
            "timing": {
                "total_time": 663.664,
                "total_inference_time": 663.51,
                "average_inference_time": 2.697,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.656084656084656,
                "precision": 0.8118811881188119,
                "recall": 0.640625,
                "f1_score": 0.7161572052401747,
                "true_positives": 82,
                "false_positives": 19,
                "false_negatives": 46,
                "true_negatives": 42
            },
            "timing": {
                "total_time": 506.539,
                "total_inference_time": 506.422,
                "average_inference_time": 2.679,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "minicpm-v",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.40860215053763443,
                "precision": 1.0,
                "recall": 0.28104575163398693,
                "f1_score": 0.4387755102040817,
                "true_positives": 43,
                "false_positives": 0,
                "false_negatives": 110,
                "true_negatives": 33
            },
            "timing": {
                "total_time": 499.96,
                "total_inference_time": 499.844,
                "average_inference_time": 2.687,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "deepseek-r1:7b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.4844961240310077,
                "precision": 0.4845360824742268,
                "recall": 0.36153846153846153,
                "f1_score": 0.41409691629955947,
                "true_positives": 47,
                "false_positives": 50,
                "false_negatives": 83,
                "true_negatives": 78
            },
            "timing": {
                "total_time": 710.849,
                "total_inference_time": 710.698,
                "average_inference_time": 2.755,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "deepseek-r1:7b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.35365853658536583,
                "precision": 0.6883116883116883,
                "recall": 0.28191489361702127,
                "f1_score": 0.39999999999999997,
                "true_positives": 53,
                "false_positives": 24,
                "false_negatives": 135,
                "true_negatives": 34
            },
            "timing": {
                "total_time": 654.085,
                "total_inference_time": 653.934,
                "average_inference_time": 2.658,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "deepseek-r1:7b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.455026455026455,
                "precision": 0.6582278481012658,
                "recall": 0.40625,
                "f1_score": 0.5024154589371981,
                "true_positives": 52,
                "false_positives": 27,
                "false_negatives": 76,
                "true_negatives": 34
            },
            "timing": {
                "total_time": 503.964,
                "total_inference_time": 503.854,
                "average_inference_time": 2.666,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "deepseek-r1:7b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.40860215053763443,
                "precision": 0.8115942028985508,
                "recall": 0.3660130718954248,
                "f1_score": 0.5045045045045045,
                "true_positives": 56,
                "false_positives": 13,
                "false_negatives": 97,
                "true_negatives": 20
            },
            "timing": {
                "total_time": 493.625,
                "total_inference_time": 493.525,
                "average_inference_time": 2.653,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "deepseek-r1:7b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.5155038759689923,
                "precision": 0.5177304964539007,
                "recall": 0.5615384615384615,
                "f1_score": 0.5387453874538746,
                "true_positives": 73,
                "false_positives": 68,
                "false_negatives": 57,
                "true_negatives": 60
            },
            "timing": {
                "total_time": 667.589,
                "total_inference_time": 667.425,
                "average_inference_time": 2.587,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "deepseek-r1:7b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.5447154471544715,
                "precision": 0.7676056338028169,
                "recall": 0.5797872340425532,
                "f1_score": 0.6606060606060606,
                "true_positives": 109,
                "false_positives": 33,
                "false_negatives": 79,
                "true_negatives": 25
            },
            "timing": {
                "total_time": 632.84,
                "total_inference_time": 632.678,
                "average_inference_time": 2.572,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "deepseek-r1:7b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.48677248677248675,
                "precision": 0.6422018348623854,
                "recall": 0.546875,
                "f1_score": 0.590717299578059,
                "true_positives": 70,
                "false_positives": 39,
                "false_negatives": 58,
                "true_negatives": 22
            },
            "timing": {
                "total_time": 486.688,
                "total_inference_time": 486.572,
                "average_inference_time": 2.574,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "deepseek-r1:7b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.5591397849462365,
                "precision": 0.8256880733944955,
                "recall": 0.5882352941176471,
                "f1_score": 0.6870229007633588,
                "true_positives": 90,
                "false_positives": 19,
                "false_negatives": 63,
                "true_negatives": 14
            },
            "timing": {
                "total_time": 481.675,
                "total_inference_time": 481.565,
                "average_inference_time": 2.589,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "deepseek-r1:32b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.5,
                "precision": 0.5037593984962406,
                "recall": 0.5153846153846153,
                "f1_score": 0.5095057034220533,
                "true_positives": 67,
                "false_positives": 66,
                "false_negatives": 63,
                "true_negatives": 62
            },
            "timing": {
                "total_time": 927.676,
                "total_inference_time": 927.492,
                "average_inference_time": 3.595,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "deepseek-r1:32b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.491869918699187,
                "precision": 0.7692307692307693,
                "recall": 0.4787234042553192,
                "f1_score": 0.5901639344262296,
                "true_positives": 90,
                "false_positives": 27,
                "false_negatives": 98,
                "true_negatives": 31
            },
            "timing": {
                "total_time": 798.649,
                "total_inference_time": 798.49,
                "average_inference_time": 3.246,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "deepseek-r1:32b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.5079365079365079,
                "precision": 0.6923076923076923,
                "recall": 0.4921875,
                "f1_score": 0.5753424657534246,
                "true_positives": 63,
                "false_positives": 28,
                "false_negatives": 65,
                "true_negatives": 33
            },
            "timing": {
                "total_time": 611.249,
                "total_inference_time": 611.127,
                "average_inference_time": 3.233,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "deepseek-r1:32b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.510752688172043,
                "precision": 0.803921568627451,
                "recall": 0.5359477124183006,
                "f1_score": 0.6431372549019607,
                "true_positives": 82,
                "false_positives": 20,
                "false_negatives": 71,
                "true_negatives": 13
            },
            "timing": {
                "total_time": 605.381,
                "total_inference_time": 605.249,
                "average_inference_time": 3.254,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "deepseek-r1:32b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.5193798449612403,
                "precision": 0.5416666666666666,
                "recall": 0.3,
                "f1_score": 0.38613861386138615,
                "true_positives": 39,
                "false_positives": 33,
                "false_negatives": 91,
                "true_negatives": 95
            },
            "timing": {
                "total_time": 918.492,
                "total_inference_time": 918.337,
                "average_inference_time": 3.559,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "deepseek-r1:32b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.3861788617886179,
                "precision": 0.7605633802816901,
                "recall": 0.2872340425531915,
                "f1_score": 0.41698841698841704,
                "true_positives": 54,
                "false_positives": 17,
                "false_negatives": 134,
                "true_negatives": 41
            },
            "timing": {
                "total_time": 879.758,
                "total_inference_time": 879.6,
                "average_inference_time": 3.576,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "deepseek-r1:32b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.42328042328042326,
                "precision": 0.7111111111111111,
                "recall": 0.25,
                "f1_score": 0.3699421965317919,
                "true_positives": 32,
                "false_positives": 13,
                "false_negatives": 96,
                "true_negatives": 48
            },
            "timing": {
                "total_time": 674.86,
                "total_inference_time": 674.738,
                "average_inference_time": 3.57,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "deepseek-r1:32b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.3978494623655914,
                "precision": 0.8596491228070176,
                "recall": 0.3202614379084967,
                "f1_score": 0.4666666666666667,
                "true_positives": 49,
                "false_positives": 8,
                "false_negatives": 104,
                "true_negatives": 25
            },
            "timing": {
                "total_time": 662.461,
                "total_inference_time": 662.339,
                "average_inference_time": 3.561,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.7015503875968992,
                "precision": 0.6331658291457286,
                "recall": 0.9692307692307692,
                "f1_score": 0.7659574468085106,
                "true_positives": 126,
                "false_positives": 73,
                "false_negatives": 4,
                "true_negatives": 55
            },
            "timing": {
                "total_time": 1052.61,
                "total_inference_time": 1052.431,
                "average_inference_time": 4.079,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.9227642276422764,
                "precision": 0.9720670391061452,
                "recall": 0.925531914893617,
                "f1_score": 0.9482288828337874,
                "true_positives": 174,
                "false_positives": 5,
                "false_negatives": 14,
                "true_negatives": 53
            },
            "timing": {
                "total_time": 833.109,
                "total_inference_time": 832.95,
                "average_inference_time": 3.386,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.7513227513227513,
                "precision": 0.7913669064748201,
                "recall": 0.859375,
                "f1_score": 0.8239700374531835,
                "true_positives": 110,
                "false_positives": 29,
                "false_negatives": 18,
                "true_negatives": 32
            },
            "timing": {
                "total_time": 640.201,
                "total_inference_time": 640.085,
                "average_inference_time": 3.387,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.6236559139784946,
                "precision": 0.8738738738738738,
                "recall": 0.6339869281045751,
                "f1_score": 0.7348484848484848,
                "true_positives": 97,
                "false_positives": 14,
                "false_negatives": 56,
                "true_negatives": 19
            },
            "timing": {
                "total_time": 630.579,
                "total_inference_time": 630.448,
                "average_inference_time": 3.39,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.6976744186046512,
                "precision": 0.6444444444444445,
                "recall": 0.8923076923076924,
                "f1_score": 0.7483870967741936,
                "true_positives": 116,
                "false_positives": 64,
                "false_negatives": 14,
                "true_negatives": 64
            },
            "timing": {
                "total_time": 888.239,
                "total_inference_time": 888.061,
                "average_inference_time": 3.442,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.8739837398373984,
                "precision": 0.9937106918238994,
                "recall": 0.8404255319148937,
                "f1_score": 0.9106628242074929,
                "true_positives": 158,
                "false_positives": 1,
                "false_negatives": 30,
                "true_negatives": 57
            },
            "timing": {
                "total_time": 841.464,
                "total_inference_time": 841.288,
                "average_inference_time": 3.42,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.6772486772486772,
                "precision": 0.8073394495412844,
                "recall": 0.6875,
                "f1_score": 0.7426160337552743,
                "true_positives": 88,
                "false_positives": 21,
                "false_negatives": 40,
                "true_negatives": 40
            },
            "timing": {
                "total_time": 648.227,
                "total_inference_time": 648.089,
                "average_inference_time": 3.429,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "gemma3:27b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.5645161290322581,
                "precision": 0.9864864864864865,
                "recall": 0.477124183006536,
                "f1_score": 0.6431718061674009,
                "true_positives": 73,
                "false_positives": 1,
                "false_negatives": 80,
                "true_negatives": 32
            },
            "timing": {
                "total_time": 635.822,
                "total_inference_time": 635.686,
                "average_inference_time": 3.418,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.6821705426356589,
                "precision": 0.6188118811881188,
                "recall": 0.9615384615384616,
                "f1_score": 0.7530120481927711,
                "true_positives": 125,
                "false_positives": 77,
                "false_negatives": 5,
                "true_negatives": 51
            },
            "timing": {
                "total_time": 891.878,
                "total_inference_time": 891.699,
                "average_inference_time": 3.456,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.9227642276422764,
                "precision": 0.9567567567567568,
                "recall": 0.9414893617021277,
                "f1_score": 0.9490616621983915,
                "true_positives": 177,
                "false_positives": 8,
                "false_negatives": 11,
                "true_negatives": 50
            },
            "timing": {
                "total_time": 762.379,
                "total_inference_time": 762.231,
                "average_inference_time": 3.099,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.7777777777777778,
                "precision": 0.7866666666666666,
                "recall": 0.921875,
                "f1_score": 0.8489208633093526,
                "true_positives": 118,
                "false_positives": 32,
                "false_negatives": 10,
                "true_negatives": 29
            },
            "timing": {
                "total_time": 587.406,
                "total_inference_time": 587.271,
                "average_inference_time": 3.107,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.7150537634408602,
                "precision": 0.8846153846153846,
                "recall": 0.7516339869281046,
                "f1_score": 0.8127208480565371,
                "true_positives": 115,
                "false_positives": 15,
                "false_negatives": 38,
                "true_negatives": 18
            },
            "timing": {
                "total_time": 576.418,
                "total_inference_time": 576.305,
                "average_inference_time": 3.098,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "person",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 258,
            "metrics": {
                "accuracy": 0.4418604651162791,
                "precision": 0.39705882352941174,
                "recall": 0.2076923076923077,
                "f1_score": 0.27272727272727276,
                "true_positives": 27,
                "false_positives": 41,
                "false_negatives": 103,
                "true_negatives": 87
            },
            "timing": {
                "total_time": 818.5,
                "total_inference_time": 818.338,
                "average_inference_time": 3.172,
                "successful_predictions": 258
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/helmet_sample_output_crops",
            "category": "head_helmet",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 246,
            "metrics": {
                "accuracy": 0.7032520325203252,
                "precision": 0.896551724137931,
                "recall": 0.6914893617021277,
                "f1_score": 0.7807807807807808,
                "true_positives": 130,
                "false_positives": 15,
                "false_negatives": 58,
                "true_negatives": 43
            },
            "timing": {
                "total_time": 777.117,
                "total_inference_time": 776.956,
                "average_inference_time": 3.158,
                "successful_predictions": 246
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "person",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 189,
            "metrics": {
                "accuracy": 0.6613756613756614,
                "precision": 0.7580645161290323,
                "recall": 0.734375,
                "f1_score": 0.7460317460317459,
                "true_positives": 94,
                "false_positives": 30,
                "false_negatives": 34,
                "true_negatives": 31
            },
            "timing": {
                "total_time": 597.807,
                "total_inference_time": 597.685,
                "average_inference_time": 3.162,
                "successful_predictions": 189
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/lng_output_crops",
            "category": "head_helmet",
            "model": "gemma3:12b",
            "prompt_file": "prompts/test-prompts-en.md",
            "experiment_type": "crop",
            "total_samples": 186,
            "metrics": {
                "accuracy": 0.5591397849462365,
                "precision": 0.8446601941747572,
                "recall": 0.5686274509803921,
                "f1_score": 0.6796875,
                "true_positives": 87,
                "false_positives": 16,
                "false_negatives": 66,
                "true_negatives": 17
            },
            "timing": {
                "total_time": 587.253,
                "total_inference_time": 587.129,
                "average_inference_time": 3.157,
                "successful_predictions": 186
            },
            "prompt_config": "test-prompts-en"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "deepseek-janus",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.11924119241192412,
                    "precision": 0.12394366197183099,
                    "recall": 0.7586206896551724,
                    "f1_score": 0.21307506053268765,
                    "true_positives": 44,
                    "false_positives": 311,
                    "false_negatives": 14,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.30113636363636365,
                    "precision": 0.3925925925925926,
                    "recall": 0.5638297872340425,
                    "f1_score": 0.46288209606986896,
                    "true_positives": 106,
                    "false_positives": 164,
                    "false_negatives": 82,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.38278388278388276,
                    "precision": 0.42052313883299797,
                    "recall": 0.810077519379845,
                    "f1_score": 0.5536423841059603,
                    "true_positives": 209,
                    "false_positives": 288,
                    "false_negatives": 49,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.24561403508771928,
                    "precision": 0.34146341463414637,
                    "recall": 0.4666666666666667,
                    "f1_score": 0.3943661971830986,
                    "true_positives": 14,
                    "false_positives": 27,
                    "false_negatives": 16,
                    "true_negatives": 23
                }
            },
            "timing": {
                "total_time": 234.946,
                "total_inference_time": 234.278,
                "average_inference_time": 2.928,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "deepseek-janus",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.06232294617563739,
                    "precision": 0.06432748538011696,
                    "recall": 0.6666666666666666,
                    "f1_score": 0.11733333333333335,
                    "true_positives": 22,
                    "false_positives": 320,
                    "false_negatives": 11,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.2753623188405797,
                    "precision": 0.3310104529616725,
                    "recall": 0.6209150326797386,
                    "f1_score": 0.4318181818181819,
                    "true_positives": 95,
                    "false_positives": 192,
                    "false_negatives": 58,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.32142857142857145,
                    "precision": 0.33962264150943394,
                    "recall": 0.8571428571428571,
                    "f1_score": 0.48648648648648646,
                    "true_positives": 162,
                    "false_positives": 315,
                    "false_negatives": 27,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.38181818181818183,
                    "precision": 0.4883720930232558,
                    "recall": 0.6363636363636364,
                    "f1_score": 0.5526315789473685,
                    "true_positives": 21,
                    "false_positives": 22,
                    "false_negatives": 12,
                    "true_negatives": 25
                }
            },
            "timing": {
                "total_time": 220.294,
                "total_inference_time": 219.642,
                "average_inference_time": 2.746,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "deepseek-janus",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.13157894736842105,
                    "precision": 0.1440329218106996,
                    "recall": 0.603448275862069,
                    "f1_score": 0.23255813953488372,
                    "true_positives": 35,
                    "false_positives": 208,
                    "false_negatives": 23,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.331002331002331,
                    "precision": 0.370757180156658,
                    "recall": 0.7553191489361702,
                    "f1_score": 0.49737302977232933,
                    "true_positives": 142,
                    "false_positives": 241,
                    "false_negatives": 46,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.4072727272727273,
                    "precision": 0.43410852713178294,
                    "recall": 0.8682170542635659,
                    "f1_score": 0.5788113695090439,
                    "true_positives": 224,
                    "false_positives": 292,
                    "false_negatives": 34,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.28846153846153844,
                    "precision": 0.40540540540540543,
                    "recall": 0.5,
                    "f1_score": 0.4477611940298507,
                    "true_positives": 15,
                    "false_positives": 22,
                    "false_negatives": 15,
                    "true_negatives": 28
                }
            },
            "timing": {
                "total_time": 206.42,
                "total_inference_time": 206.34,
                "average_inference_time": 2.579,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "deepseek-janus",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.12295081967213115,
                    "precision": 0.12448132780082988,
                    "recall": 0.9090909090909091,
                    "f1_score": 0.218978102189781,
                    "true_positives": 30,
                    "false_positives": 211,
                    "false_negatives": 3,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.2650918635170604,
                    "precision": 0.3069908814589666,
                    "recall": 0.6601307189542484,
                    "f1_score": 0.4190871369294606,
                    "true_positives": 101,
                    "false_positives": 228,
                    "false_negatives": 52,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.3346693386773547,
                    "precision": 0.350104821802935,
                    "recall": 0.8835978835978836,
                    "f1_score": 0.5015015015015014,
                    "true_positives": 167,
                    "false_positives": 310,
                    "false_negatives": 22,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.25,
                    "precision": 0.3783783783783784,
                    "recall": 0.42424242424242425,
                    "f1_score": 0.4000000000000001,
                    "true_positives": 14,
                    "false_positives": 23,
                    "false_negatives": 19,
                    "true_negatives": 24
                }
            },
            "timing": {
                "total_time": 209.93,
                "total_inference_time": 209.846,
                "average_inference_time": 2.623,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "qwen2.5-vl-32b",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.16279069767441862,
                    "precision": 0.22826086956521738,
                    "recall": 0.3620689655172414,
                    "f1_score": 0.27999999999999997,
                    "true_positives": 21,
                    "false_positives": 71,
                    "false_negatives": 37,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.2815533980582524,
                    "precision": 0.7631578947368421,
                    "recall": 0.30851063829787234,
                    "f1_score": 0.4393939393939394,
                    "true_positives": 58,
                    "false_positives": 18,
                    "false_negatives": 130,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.42,
                    "precision": 0.75,
                    "recall": 0.4883720930232558,
                    "f1_score": 0.5915492957746479,
                    "true_positives": 126,
                    "false_positives": 42,
                    "false_negatives": 132,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.3050847457627119,
                    "precision": 0.3829787234042553,
                    "recall": 0.6,
                    "f1_score": 0.4675324675324675,
                    "true_positives": 18,
                    "false_positives": 29,
                    "false_negatives": 12,
                    "true_negatives": 21
                }
            },
            "timing": {
                "total_time": 527.947,
                "total_inference_time": 527.875,
                "average_inference_time": 6.598,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "qwen2.5-vl-32b",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.13861386138613863,
                    "precision": 0.17073170731707318,
                    "recall": 0.42424242424242425,
                    "f1_score": 0.24347826086956523,
                    "true_positives": 14,
                    "false_positives": 68,
                    "false_negatives": 19,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.30412371134020616,
                    "precision": 0.59,
                    "recall": 0.38562091503267976,
                    "f1_score": 0.46640316205533594,
                    "true_positives": 59,
                    "false_positives": 41,
                    "false_negatives": 94,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.4357976653696498,
                    "precision": 0.6222222222222222,
                    "recall": 0.5925925925925926,
                    "f1_score": 0.6070460704607046,
                    "true_positives": 112,
                    "false_positives": 68,
                    "false_negatives": 77,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.2641509433962264,
                    "precision": 0.4117647058823529,
                    "recall": 0.42424242424242425,
                    "f1_score": 0.41791044776119407,
                    "true_positives": 14,
                    "false_positives": 20,
                    "false_negatives": 19,
                    "true_negatives": 27
                }
            },
            "timing": {
                "total_time": 389.512,
                "total_inference_time": 389.429,
                "average_inference_time": 4.868,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "qwen2.5-vl-32b",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.05223880597014925,
                    "precision": 0.0625,
                    "recall": 0.2413793103448276,
                    "f1_score": 0.09929078014184396,
                    "true_positives": 14,
                    "false_positives": 210,
                    "false_negatives": 44,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.1687116564417178,
                    "precision": 0.2849740932642487,
                    "recall": 0.2925531914893617,
                    "f1_score": 0.2887139107611549,
                    "true_positives": 55,
                    "false_positives": 138,
                    "false_negatives": 133,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.20052083333333334,
                    "precision": 0.3793103448275862,
                    "recall": 0.29844961240310075,
                    "f1_score": 0.33405639913232105,
                    "true_positives": 77,
                    "false_positives": 126,
                    "false_negatives": 181,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.37037037037037035,
                    "precision": 0.45454545454545453,
                    "recall": 0.6666666666666666,
                    "f1_score": 0.5405405405405405,
                    "true_positives": 20,
                    "false_positives": 24,
                    "false_negatives": 10,
                    "true_negatives": 26
                }
            },
            "timing": {
                "total_time": 724.174,
                "total_inference_time": 724.093,
                "average_inference_time": 9.051,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "qwen2.5-vl-32b",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.038314176245210725,
                    "precision": 0.04201680672268908,
                    "recall": 0.30303030303030304,
                    "f1_score": 0.07380073800738009,
                    "true_positives": 10,
                    "false_positives": 228,
                    "false_negatives": 23,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.1580547112462006,
                    "precision": 0.22807017543859648,
                    "recall": 0.33986928104575165,
                    "f1_score": 0.27296587926509186,
                    "true_positives": 52,
                    "false_positives": 176,
                    "false_negatives": 101,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.19160104986876642,
                    "precision": 0.27547169811320754,
                    "recall": 0.3862433862433862,
                    "f1_score": 0.32158590308370044,
                    "true_positives": 73,
                    "false_positives": 192,
                    "false_negatives": 116,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.265625,
                    "precision": 0.3541666666666667,
                    "recall": 0.5151515151515151,
                    "f1_score": 0.41975308641975306,
                    "true_positives": 17,
                    "false_positives": 31,
                    "false_negatives": 16,
                    "true_negatives": 16
                }
            },
            "timing": {
                "total_time": 726.499,
                "total_inference_time": 726.409,
                "average_inference_time": 9.08,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "minicpm-o",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.5595238095238095,
                    "precision": 0.6438356164383562,
                    "recall": 0.8103448275862069,
                    "f1_score": 0.7175572519083969,
                    "true_positives": 47,
                    "false_positives": 26,
                    "false_negatives": 11,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.7878787878787878,
                    "precision": 0.9397590361445783,
                    "recall": 0.8297872340425532,
                    "f1_score": 0.8813559322033898,
                    "true_positives": 156,
                    "false_positives": 10,
                    "false_negatives": 32,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.8629629629629629,
                    "precision": 0.9510204081632653,
                    "recall": 0.9031007751937985,
                    "f1_score": 0.9264413518886679,
                    "true_positives": 233,
                    "false_positives": 12,
                    "false_negatives": 25,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.6444444444444445,
                    "precision": 0.6590909090909091,
                    "recall": 0.9666666666666667,
                    "f1_score": 0.7837837837837838,
                    "true_positives": 29,
                    "false_positives": 15,
                    "false_negatives": 1,
                    "true_negatives": 35
                }
            },
            "timing": {
                "total_time": 373.516,
                "total_inference_time": 373.447,
                "average_inference_time": 4.668,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "minicpm-o",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.2037037037037037,
                    "precision": 0.34375,
                    "recall": 0.3333333333333333,
                    "f1_score": 0.3384615384615385,
                    "true_positives": 11,
                    "false_positives": 21,
                    "false_negatives": 22,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.6353591160220995,
                    "precision": 0.8041958041958042,
                    "recall": 0.7516339869281046,
                    "f1_score": 0.777027027027027,
                    "true_positives": 115,
                    "false_positives": 28,
                    "false_negatives": 38,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.7534246575342466,
                    "precision": 0.8461538461538461,
                    "recall": 0.873015873015873,
                    "f1_score": 0.859375,
                    "true_positives": 165,
                    "false_positives": 30,
                    "false_negatives": 24,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.25,
                    "precision": 0.40625,
                    "recall": 0.3939393939393939,
                    "f1_score": 0.4,
                    "true_positives": 13,
                    "false_positives": 19,
                    "false_negatives": 20,
                    "true_negatives": 28
                }
            },
            "timing": {
                "total_time": 397.906,
                "total_inference_time": 397.819,
                "average_inference_time": 4.973,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "minicpm-o",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.31313131313131315,
                    "precision": 0.4305555555555556,
                    "recall": 0.5344827586206896,
                    "f1_score": 0.47692307692307695,
                    "true_positives": 31,
                    "false_positives": 41,
                    "false_negatives": 27,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.7894736842105263,
                    "precision": 0.8870967741935484,
                    "recall": 0.8776595744680851,
                    "f1_score": 0.8823529411764706,
                    "true_positives": 165,
                    "false_positives": 21,
                    "false_negatives": 23,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.8333333333333334,
                    "precision": 0.8888888888888888,
                    "recall": 0.9302325581395349,
                    "f1_score": 0.9090909090909092,
                    "true_positives": 240,
                    "false_positives": 30,
                    "false_negatives": 18,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.40625,
                    "precision": 0.43333333333333335,
                    "recall": 0.8666666666666667,
                    "f1_score": 0.5777777777777778,
                    "true_positives": 26,
                    "false_positives": 34,
                    "false_negatives": 4,
                    "true_negatives": 16
                }
            },
            "timing": {
                "total_time": 296.316,
                "total_inference_time": 296.245,
                "average_inference_time": 3.703,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "minicpm-o",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.37037037037037035,
                    "precision": 0.38461538461538464,
                    "recall": 0.9090909090909091,
                    "f1_score": 0.5405405405405405,
                    "true_positives": 30,
                    "false_positives": 48,
                    "false_negatives": 3,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.6608187134502924,
                    "precision": 0.8625954198473282,
                    "recall": 0.738562091503268,
                    "f1_score": 0.7957746478873239,
                    "true_positives": 113,
                    "false_positives": 18,
                    "false_negatives": 40,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.7479338842975206,
                    "precision": 0.7735042735042735,
                    "recall": 0.9576719576719577,
                    "f1_score": 0.8557919621749409,
                    "true_positives": 181,
                    "false_positives": 53,
                    "false_negatives": 8,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.4225352112676056,
                    "precision": 0.4411764705882353,
                    "recall": 0.9090909090909091,
                    "f1_score": 0.5940594059405941,
                    "true_positives": 30,
                    "false_positives": 38,
                    "false_negatives": 3,
                    "true_negatives": 9
                }
            },
            "timing": {
                "total_time": 395.084,
                "total_inference_time": 394.997,
                "average_inference_time": 4.937,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "minicpm-v",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.44537815126050423,
                    "precision": 0.4649122807017544,
                    "recall": 0.9137931034482759,
                    "f1_score": 0.6162790697674418,
                    "true_positives": 53,
                    "false_positives": 61,
                    "false_negatives": 5,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.7673267326732673,
                    "precision": 0.9171597633136095,
                    "recall": 0.824468085106383,
                    "f1_score": 0.8683473389355743,
                    "true_positives": 155,
                    "false_positives": 14,
                    "false_negatives": 33,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.8073089700996677,
                    "precision": 0.8496503496503497,
                    "recall": 0.9418604651162791,
                    "f1_score": 0.8933823529411765,
                    "true_positives": 243,
                    "false_positives": 43,
                    "false_negatives": 15,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.5471698113207547,
                    "precision": 0.5576923076923077,
                    "recall": 0.9666666666666667,
                    "f1_score": 0.7073170731707317,
                    "true_positives": 29,
                    "false_positives": 23,
                    "false_negatives": 1,
                    "true_negatives": 27
                }
            },
            "timing": {
                "total_time": 292.119,
                "total_inference_time": 292.05,
                "average_inference_time": 3.651,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "minicpm-v",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.12,
                    "precision": 0.1518987341772152,
                    "recall": 0.36363636363636365,
                    "f1_score": 0.21428571428571427,
                    "true_positives": 12,
                    "false_positives": 67,
                    "false_negatives": 21,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.6428571428571429,
                    "precision": 0.8780487804878049,
                    "recall": 0.7058823529411765,
                    "f1_score": 0.782608695652174,
                    "true_positives": 108,
                    "false_positives": 15,
                    "false_negatives": 45,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.667953667953668,
                    "precision": 0.7119341563786008,
                    "recall": 0.9153439153439153,
                    "f1_score": 0.8009259259259259,
                    "true_positives": 173,
                    "false_positives": 70,
                    "false_negatives": 16,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.30158730158730157,
                    "precision": 0.3877551020408163,
                    "recall": 0.5757575757575758,
                    "f1_score": 0.4634146341463415,
                    "true_positives": 19,
                    "false_positives": 30,
                    "false_negatives": 14,
                    "true_negatives": 17
                }
            },
            "timing": {
                "total_time": 356.104,
                "total_inference_time": 356.022,
                "average_inference_time": 4.45,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "minicpm-v",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.1797752808988764,
                    "precision": 0.3404255319148936,
                    "recall": 0.27586206896551724,
                    "f1_score": 0.3047619047619048,
                    "true_positives": 16,
                    "false_positives": 31,
                    "false_negatives": 42,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.729957805907173,
                    "precision": 0.7792792792792793,
                    "recall": 0.9202127659574468,
                    "f1_score": 0.8439024390243902,
                    "true_positives": 173,
                    "false_positives": 49,
                    "false_negatives": 15,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.7927631578947368,
                    "precision": 0.8397212543554007,
                    "recall": 0.9341085271317829,
                    "f1_score": 0.8844036697247706,
                    "true_positives": 241,
                    "false_positives": 46,
                    "false_negatives": 17,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.26,
                    "precision": 0.3939393939393939,
                    "recall": 0.43333333333333335,
                    "f1_score": 0.4126984126984127,
                    "true_positives": 13,
                    "false_positives": 20,
                    "false_negatives": 17,
                    "true_negatives": 30
                }
            },
            "timing": {
                "total_time": 263.245,
                "total_inference_time": 263.181,
                "average_inference_time": 3.29,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "minicpm-v",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.2345679012345679,
                    "precision": 0.2835820895522388,
                    "recall": 0.5757575757575758,
                    "f1_score": 0.38,
                    "true_positives": 19,
                    "false_positives": 48,
                    "false_negatives": 14,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.3931888544891641,
                    "precision": 0.4276094276094276,
                    "recall": 0.8300653594771242,
                    "f1_score": 0.5644444444444444,
                    "true_positives": 127,
                    "false_positives": 170,
                    "false_negatives": 26,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.47643979057591623,
                    "precision": 0.48533333333333334,
                    "recall": 0.9629629629629629,
                    "f1_score": 0.6453900709219859,
                    "true_positives": 182,
                    "false_positives": 193,
                    "false_negatives": 7,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.3333333333333333,
                    "precision": 0.425531914893617,
                    "recall": 0.6060606060606061,
                    "f1_score": 0.5,
                    "true_positives": 20,
                    "false_positives": 27,
                    "false_negatives": 13,
                    "true_negatives": 20
                }
            },
            "timing": {
                "total_time": 353.403,
                "total_inference_time": 353.313,
                "average_inference_time": 4.416,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "deepseek-r1:7b",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.18333333333333332,
                    "precision": 0.2619047619047619,
                    "recall": 0.3793103448275862,
                    "f1_score": 0.30985915492957744,
                    "true_positives": 22,
                    "false_positives": 62,
                    "false_negatives": 36,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.35842293906810035,
                    "precision": 0.5235602094240838,
                    "recall": 0.5319148936170213,
                    "f1_score": 0.5277044854881267,
                    "true_positives": 100,
                    "false_positives": 91,
                    "false_negatives": 88,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.38726790450928383,
                    "precision": 0.5509433962264151,
                    "recall": 0.5658914728682171,
                    "f1_score": 0.5583173996175909,
                    "true_positives": 146,
                    "false_positives": 119,
                    "false_negatives": 112,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.0975609756097561,
                    "precision": 0.26666666666666666,
                    "recall": 0.13333333333333333,
                    "f1_score": 0.17777777777777776,
                    "true_positives": 4,
                    "false_positives": 11,
                    "false_negatives": 26,
                    "true_negatives": 39
                }
            },
            "timing": {
                "total_time": 254.502,
                "total_inference_time": 254.441,
                "average_inference_time": 3.181,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "deepseek-r1:7b",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.1171875,
                    "precision": 0.13636363636363635,
                    "recall": 0.45454545454545453,
                    "f1_score": 0.2097902097902098,
                    "true_positives": 15,
                    "false_positives": 95,
                    "false_negatives": 18,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.3409090909090909,
                    "precision": 0.40384615384615385,
                    "recall": 0.6862745098039216,
                    "f1_score": 0.5084745762711864,
                    "true_positives": 105,
                    "false_positives": 155,
                    "false_negatives": 48,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.369620253164557,
                    "precision": 0.4147727272727273,
                    "recall": 0.7724867724867724,
                    "f1_score": 0.5397412199630315,
                    "true_positives": 146,
                    "false_positives": 206,
                    "false_negatives": 43,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.075,
                    "precision": 0.3,
                    "recall": 0.09090909090909091,
                    "f1_score": 0.13953488372093023,
                    "true_positives": 3,
                    "false_positives": 7,
                    "false_negatives": 30,
                    "true_negatives": 40
                }
            },
            "timing": {
                "total_time": 236.872,
                "total_inference_time": 236.791,
                "average_inference_time": 2.96,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "deepseek-r1:7b",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.1581196581196581,
                    "precision": 0.17370892018779344,
                    "recall": 0.6379310344827587,
                    "f1_score": 0.2730627306273063,
                    "true_positives": 37,
                    "false_positives": 176,
                    "false_negatives": 21,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.4576271186440678,
                    "precision": 0.49390243902439024,
                    "recall": 0.8617021276595744,
                    "f1_score": 0.627906976744186,
                    "true_positives": 162,
                    "false_positives": 166,
                    "false_negatives": 26,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.4270650263620387,
                    "precision": 0.43862815884476536,
                    "recall": 0.9418604651162791,
                    "f1_score": 0.5985221674876848,
                    "true_positives": 243,
                    "false_positives": 311,
                    "false_negatives": 15,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.32786885245901637,
                    "precision": 0.39215686274509803,
                    "recall": 0.6666666666666666,
                    "f1_score": 0.4938271604938271,
                    "true_positives": 20,
                    "false_positives": 31,
                    "false_negatives": 10,
                    "true_negatives": 19
                }
            },
            "timing": {
                "total_time": 228.985,
                "total_inference_time": 228.917,
                "average_inference_time": 2.861,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "deepseek-r1:7b",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.10317460317460317,
                    "precision": 0.10612244897959183,
                    "recall": 0.7878787878787878,
                    "f1_score": 0.18705035971223022,
                    "true_positives": 26,
                    "false_positives": 219,
                    "false_negatives": 7,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.39244186046511625,
                    "precision": 0.41411042944785276,
                    "recall": 0.8823529411764706,
                    "f1_score": 0.5636743215031316,
                    "true_positives": 135,
                    "false_positives": 191,
                    "false_negatives": 18,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.3216168717047452,
                    "precision": 0.325044404973357,
                    "recall": 0.9682539682539683,
                    "f1_score": 0.48670212765957444,
                    "true_positives": 183,
                    "false_positives": 380,
                    "false_negatives": 6,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.38235294117647056,
                    "precision": 0.4262295081967213,
                    "recall": 0.7878787878787878,
                    "f1_score": 0.5531914893617021,
                    "true_positives": 26,
                    "false_positives": 35,
                    "false_negatives": 7,
                    "true_negatives": 12
                }
            },
            "timing": {
                "total_time": 232.157,
                "total_inference_time": 232.076,
                "average_inference_time": 2.901,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "deepseek-r1:32b",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.16312056737588654,
                    "precision": 0.2169811320754717,
                    "recall": 0.39655172413793105,
                    "f1_score": 0.28048780487804875,
                    "true_positives": 23,
                    "false_positives": 83,
                    "false_negatives": 35,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.3951219512195122,
                    "precision": 0.826530612244898,
                    "recall": 0.4308510638297872,
                    "f1_score": 0.5664335664335665,
                    "true_positives": 81,
                    "false_positives": 17,
                    "false_negatives": 107,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.4696485623003195,
                    "precision": 0.7277227722772277,
                    "recall": 0.5697674418604651,
                    "f1_score": 0.6391304347826088,
                    "true_positives": 147,
                    "false_positives": 55,
                    "false_negatives": 111,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.27419354838709675,
                    "precision": 0.3469387755102041,
                    "recall": 0.5666666666666667,
                    "f1_score": 0.43037974683544306,
                    "true_positives": 17,
                    "false_positives": 32,
                    "false_negatives": 13,
                    "true_negatives": 18
                }
            },
            "timing": {
                "total_time": 384.24,
                "total_inference_time": 384.188,
                "average_inference_time": 4.802,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "deepseek-r1:32b",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.16535433070866143,
                    "precision": 0.1826086956521739,
                    "recall": 0.6363636363636364,
                    "f1_score": 0.28378378378378377,
                    "true_positives": 21,
                    "false_positives": 94,
                    "false_negatives": 12,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.36813186813186816,
                    "precision": 0.6979166666666666,
                    "recall": 0.43790849673202614,
                    "f1_score": 0.538152610441767,
                    "true_positives": 67,
                    "false_positives": 29,
                    "false_negatives": 86,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.44244604316546765,
                    "precision": 0.5801886792452831,
                    "recall": 0.6507936507936508,
                    "f1_score": 0.6134663341645885,
                    "true_positives": 123,
                    "false_positives": 89,
                    "false_negatives": 66,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.3333333333333333,
                    "precision": 0.4117647058823529,
                    "recall": 0.6363636363636364,
                    "f1_score": 0.5,
                    "true_positives": 21,
                    "false_positives": 30,
                    "false_negatives": 12,
                    "true_negatives": 17
                }
            },
            "timing": {
                "total_time": 314.308,
                "total_inference_time": 314.233,
                "average_inference_time": 3.928,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "deepseek-r1:32b",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.10869565217391304,
                    "precision": 0.22727272727272727,
                    "recall": 0.1724137931034483,
                    "f1_score": 0.19607843137254902,
                    "true_positives": 10,
                    "false_positives": 34,
                    "false_negatives": 48,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.47959183673469385,
                    "precision": 0.5708502024291497,
                    "recall": 0.75,
                    "f1_score": 0.6482758620689655,
                    "true_positives": 141,
                    "false_positives": 106,
                    "false_negatives": 47,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.5335195530726257,
                    "precision": 0.6563573883161512,
                    "recall": 0.7403100775193798,
                    "f1_score": 0.6958105646630237,
                    "true_positives": 191,
                    "false_positives": 100,
                    "false_negatives": 67,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.1568627450980392,
                    "precision": 0.27586206896551724,
                    "recall": 0.26666666666666666,
                    "f1_score": 0.2711864406779661,
                    "true_positives": 8,
                    "false_positives": 21,
                    "false_negatives": 22,
                    "true_negatives": 29
                }
            },
            "timing": {
                "total_time": 312.28,
                "total_inference_time": 312.214,
                "average_inference_time": 3.903,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "deepseek-r1:32b",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.1323529411764706,
                    "precision": 0.20454545454545456,
                    "recall": 0.2727272727272727,
                    "f1_score": 0.23376623376623376,
                    "true_positives": 9,
                    "false_positives": 35,
                    "false_negatives": 24,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.4980544747081712,
                    "precision": 0.5517241379310345,
                    "recall": 0.8366013071895425,
                    "f1_score": 0.664935064935065,
                    "true_positives": 128,
                    "false_positives": 104,
                    "false_negatives": 25,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.5245901639344263,
                    "precision": 0.5797101449275363,
                    "recall": 0.8465608465608465,
                    "f1_score": 0.6881720430107527,
                    "true_positives": 160,
                    "false_positives": 116,
                    "false_negatives": 29,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.17307692307692307,
                    "precision": 0.32142857142857145,
                    "recall": 0.2727272727272727,
                    "f1_score": 0.2950819672131148,
                    "true_positives": 9,
                    "false_positives": 19,
                    "false_negatives": 24,
                    "true_negatives": 28
                }
            },
            "timing": {
                "total_time": 314.086,
                "total_inference_time": 314.004,
                "average_inference_time": 3.925,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "gemma3:27b",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.48148148148148145,
                    "precision": 0.5098039215686274,
                    "recall": 0.896551724137931,
                    "f1_score": 0.65,
                    "true_positives": 52,
                    "false_positives": 50,
                    "false_negatives": 6,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.7040816326530612,
                    "precision": 0.9452054794520548,
                    "recall": 0.7340425531914894,
                    "f1_score": 0.8263473053892215,
                    "true_positives": 138,
                    "false_positives": 8,
                    "false_negatives": 50,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.8534798534798534,
                    "precision": 0.9395161290322581,
                    "recall": 0.9031007751937985,
                    "f1_score": 0.9209486166007905,
                    "true_positives": 233,
                    "false_positives": 15,
                    "false_negatives": 25,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.48333333333333334,
                    "precision": 0.4915254237288136,
                    "recall": 0.9666666666666667,
                    "f1_score": 0.651685393258427,
                    "true_positives": 29,
                    "false_positives": 30,
                    "false_negatives": 1,
                    "true_negatives": 20
                }
            },
            "timing": {
                "total_time": 521.153,
                "total_inference_time": 521.081,
                "average_inference_time": 6.514,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "gemma3:27b",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.27692307692307694,
                    "precision": 0.36,
                    "recall": 0.5454545454545454,
                    "f1_score": 0.43373493975903615,
                    "true_positives": 18,
                    "false_positives": 32,
                    "false_negatives": 15,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.4397905759162304,
                    "precision": 0.6885245901639344,
                    "recall": 0.5490196078431373,
                    "f1_score": 0.6109090909090908,
                    "true_positives": 84,
                    "false_positives": 38,
                    "false_negatives": 69,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.5720524017467249,
                    "precision": 0.7660818713450293,
                    "recall": 0.6931216931216931,
                    "f1_score": 0.7277777777777779,
                    "true_positives": 131,
                    "false_positives": 40,
                    "false_negatives": 58,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.3157894736842105,
                    "precision": 0.42857142857142855,
                    "recall": 0.5454545454545454,
                    "f1_score": 0.4799999999999999,
                    "true_positives": 18,
                    "false_positives": 24,
                    "false_negatives": 15,
                    "true_negatives": 23
                }
            },
            "timing": {
                "total_time": 353.431,
                "total_inference_time": 353.34,
                "average_inference_time": 4.417,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "gemma3:27b",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.5164835164835165,
                    "precision": 0.5875,
                    "recall": 0.8103448275862069,
                    "f1_score": 0.6811594202898551,
                    "true_positives": 47,
                    "false_positives": 33,
                    "false_negatives": 11,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.7412935323383084,
                    "precision": 0.9197530864197531,
                    "recall": 0.7925531914893617,
                    "f1_score": 0.8514285714285714,
                    "true_positives": 149,
                    "false_positives": 13,
                    "false_negatives": 39,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.8513011152416357,
                    "precision": 0.9541666666666667,
                    "recall": 0.8875968992248062,
                    "f1_score": 0.9196787148594378,
                    "true_positives": 229,
                    "false_positives": 11,
                    "false_negatives": 29,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.5490196078431373,
                    "precision": 0.5714285714285714,
                    "recall": 0.9333333333333333,
                    "f1_score": 0.7088607594936709,
                    "true_positives": 28,
                    "false_positives": 21,
                    "false_negatives": 2,
                    "true_negatives": 29
                }
            },
            "timing": {
                "total_time": 335.357,
                "total_inference_time": 335.283,
                "average_inference_time": 4.191,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "gemma3:27b",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.3522727272727273,
                    "precision": 0.36046511627906974,
                    "recall": 0.9393939393939394,
                    "f1_score": 0.5210084033613445,
                    "true_positives": 31,
                    "false_positives": 55,
                    "false_negatives": 2,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.42346938775510207,
                    "precision": 0.6587301587301587,
                    "recall": 0.5424836601307189,
                    "f1_score": 0.5949820788530467,
                    "true_positives": 83,
                    "false_positives": 43,
                    "false_negatives": 70,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.5363984674329502,
                    "precision": 0.660377358490566,
                    "recall": 0.7407407407407407,
                    "f1_score": 0.6982543640897756,
                    "true_positives": 140,
                    "false_positives": 72,
                    "false_negatives": 49,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.4492753623188406,
                    "precision": 0.4626865671641791,
                    "recall": 0.9393939393939394,
                    "f1_score": 0.62,
                    "true_positives": 31,
                    "false_positives": 36,
                    "false_negatives": 2,
                    "true_negatives": 11
                }
            },
            "timing": {
                "total_time": 328.078,
                "total_inference_time": 327.992,
                "average_inference_time": 4.1,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "gemma3:12b",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.3565217391304348,
                    "precision": 0.41836734693877553,
                    "recall": 0.7068965517241379,
                    "f1_score": 0.5256410256410257,
                    "true_positives": 41,
                    "false_positives": 57,
                    "false_negatives": 17,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.4444444444444444,
                    "precision": 0.8979591836734694,
                    "recall": 0.46808510638297873,
                    "f1_score": 0.6153846153846153,
                    "true_positives": 88,
                    "false_positives": 10,
                    "false_negatives": 100,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.700374531835206,
                    "precision": 0.9540816326530612,
                    "recall": 0.7248062015503876,
                    "f1_score": 0.8237885462555066,
                    "true_positives": 187,
                    "false_positives": 9,
                    "false_negatives": 71,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.421875,
                    "precision": 0.4426229508196721,
                    "recall": 0.9,
                    "f1_score": 0.5934065934065934,
                    "true_positives": 27,
                    "false_positives": 34,
                    "false_negatives": 3,
                    "true_negatives": 16
                }
            },
            "timing": {
                "total_time": 316.808,
                "total_inference_time": 316.738,
                "average_inference_time": 3.959,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "gemma3:12b",
            "prompt_file": "prompts/count-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.3018867924528302,
                    "precision": 0.4444444444444444,
                    "recall": 0.48484848484848486,
                    "f1_score": 0.463768115942029,
                    "true_positives": 16,
                    "false_positives": 20,
                    "false_negatives": 17,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.4320388349514563,
                    "precision": 0.6267605633802817,
                    "recall": 0.5816993464052288,
                    "f1_score": 0.6033898305084747,
                    "true_positives": 89,
                    "false_positives": 53,
                    "false_negatives": 64,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.5291666666666667,
                    "precision": 0.7134831460674157,
                    "recall": 0.671957671957672,
                    "f1_score": 0.6920980926430518,
                    "true_positives": 127,
                    "false_positives": 51,
                    "false_negatives": 62,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.3404255319148936,
                    "precision": 0.5333333333333333,
                    "recall": 0.48484848484848486,
                    "f1_score": 0.507936507936508,
                    "true_positives": 16,
                    "false_positives": 14,
                    "false_negatives": 17,
                    "true_negatives": 33
                }
            },
            "timing": {
                "total_time": 299.732,
                "total_inference_time": 299.646,
                "average_inference_time": 3.746,
                "successful_predictions": 80
            },
            "prompt_config": "count-prompts"
        },
        {
            "dataset": "dataset/HELMET_SAMPLES_80/obj_train_data",
            "category": null,
            "model": "gemma3:12b",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.3219178082191781,
                    "precision": 0.34814814814814815,
                    "recall": 0.8103448275862069,
                    "f1_score": 0.48704663212435234,
                    "true_positives": 47,
                    "false_positives": 88,
                    "false_negatives": 11,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.5,
                    "precision": 0.8045112781954887,
                    "recall": 0.5691489361702128,
                    "f1_score": 0.6666666666666666,
                    "true_positives": 107,
                    "false_positives": 26,
                    "false_negatives": 81,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.6903225806451613,
                    "precision": 0.8045112781954887,
                    "recall": 0.8294573643410853,
                    "f1_score": 0.816793893129771,
                    "true_positives": 214,
                    "false_positives": 52,
                    "false_negatives": 44,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.375,
                    "precision": 0.375,
                    "recall": 1.0,
                    "f1_score": 0.5454545454545454,
                    "true_positives": 30,
                    "false_positives": 50,
                    "false_negatives": 0,
                    "true_negatives": 0
                }
            },
            "timing": {
                "total_time": 285.948,
                "total_inference_time": 285.875,
                "average_inference_time": 3.573,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        },
        {
            "dataset": "dataset/LNG_DATASET_SAMPLES_80/lng_train_data",
            "category": null,
            "model": "gemma3:12b",
            "prompt_file": "prompts/detect-prompts.md",
            "experiment_type": "count",
            "total_samples": 80,
            "metrics": {
                "head": {
                    "accuracy": 0.25196850393700787,
                    "precision": 0.25396825396825395,
                    "recall": 0.9696969696969697,
                    "f1_score": 0.4025157232704402,
                    "true_positives": 32,
                    "false_positives": 94,
                    "false_negatives": 1,
                    "true_negatives": 0
                },
                "helmet": {
                    "accuracy": 0.3793103448275862,
                    "precision": 0.42160278745644597,
                    "recall": 0.7908496732026143,
                    "f1_score": 0.5499999999999999,
                    "true_positives": 121,
                    "false_positives": 166,
                    "false_negatives": 32,
                    "true_negatives": 0
                },
                "person": {
                    "accuracy": 0.37906976744186044,
                    "precision": 0.4034653465346535,
                    "recall": 0.8624338624338624,
                    "f1_score": 0.5497470489038786,
                    "true_positives": 163,
                    "false_positives": 241,
                    "false_negatives": 26,
                    "true_negatives": 0
                },
                "alert": {
                    "accuracy": 0.41025641025641024,
                    "precision": 0.4155844155844156,
                    "recall": 0.9696969696969697,
                    "f1_score": 0.5818181818181818,
                    "true_positives": 32,
                    "false_positives": 45,
                    "false_negatives": 1,
                    "true_negatives": 2
                }
            },
            "timing": {
                "total_time": 294.295,
                "total_inference_time": 294.21,
                "average_inference_time": 3.678,
                "successful_predictions": 80
            },
            "prompt_config": "detect-prompts"
        }
    ]
}